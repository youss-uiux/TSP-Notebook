{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TSP Solver avec GCN + Beam Search (Kaggle - Double GPU)\n",
    "\n",
    "Ce notebook int√®gre tous les modules n√©cessaires pour r√©soudre le Travelling Salesman Problem (TSP) en utilisant un Residual Gated Graph Convolutional Network avec Beam Search.\n",
    "\n",
    "**R√©f√©rence:** [An Efficient Graph ConvNet for the Travelling Salesman Problem](https://arxiv.org/pdf/1711.07553v2.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation des d√©pendances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q tensorboardX fastprogress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Imports et configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "from fastprogress import master_bar, progress_bar\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "from scipy.sparse import SparseEfficiencyWarning\n",
    "warnings.simplefilter('ignore', SparseEfficiencyWarning)\n",
    "\n",
    "%matplotlib inline\n",
    "from IPython import get_ipython\n",
    "get_ipython().run_line_magic('config', \"InlineBackend.figure_format = 'png'\")\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Nombre de GPU: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configuration (Settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Settings(dict):\n",
    "    \"\"\"Experiment configuration options.\n",
    "    Wrapper around in-built dict class to access members through the dot operation.\n",
    "    \"\"\"\n",
    "    def __init__(self, config_dict):\n",
    "        super().__init__()\n",
    "        for key in config_dict:\n",
    "            self[key] = config_dict[key]\n",
    "\n",
    "    def __getattr__(self, attr):\n",
    "        return self[attr]\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "        return super().__setitem__(key, value)\n",
    "\n",
    "    def __setattr__(self, key, value):\n",
    "        return self.__setitem__(key, value)\n",
    "\n",
    "    __delattr__ = dict.__delitem__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Reader (Google TSP Reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotDict(dict):\n",
    "    \"\"\"Wrapper around in-built dict class to access members through the dot operation.\"\"\"\n",
    "    def __init__(self, **kwds):\n",
    "        self.update(kwds)\n",
    "        self.__dict__ = self\n",
    "\n",
    "\n",
    "class GoogleTSPReader(object):\n",
    "    \"\"\"Iterator that reads TSP dataset files and yields mini-batches.\n",
    "    Format expected as in Vinyals et al., 2015.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_nodes, num_neighbors, batch_size, filepath):\n",
    "        self.num_nodes = num_nodes\n",
    "        self.num_neighbors = num_neighbors\n",
    "        self.batch_size = batch_size\n",
    "        self.filepath = filepath\n",
    "        self.filedata = shuffle(open(filepath, \"r\").readlines())\n",
    "        self.max_iter = (len(self.filedata) // batch_size)\n",
    "\n",
    "    def __iter__(self):\n",
    "        for batch in range(self.max_iter):\n",
    "            start_idx = batch * self.batch_size\n",
    "            end_idx = (batch + 1) * self.batch_size\n",
    "            yield self.process_batch(self.filedata[start_idx:end_idx])\n",
    "\n",
    "    def process_batch(self, lines):\n",
    "        \"\"\"Helper function to convert raw lines into a mini-batch as a DotDict.\"\"\"\n",
    "        batch_edges = []\n",
    "        batch_edges_values = []\n",
    "        batch_edges_target = []\n",
    "        batch_nodes = []\n",
    "        batch_nodes_target = []\n",
    "        batch_nodes_coord = []\n",
    "        batch_tour_nodes = []\n",
    "        batch_tour_len = []\n",
    "\n",
    "        for line_num, line in enumerate(lines):\n",
    "            line = line.split(\" \")\n",
    "            nodes = np.ones(self.num_nodes)\n",
    "            nodes_coord = []\n",
    "            for idx in range(0, 2 * self.num_nodes, 2):\n",
    "                nodes_coord.append([float(line[idx]), float(line[idx + 1])])\n",
    "\n",
    "            W_val = squareform(pdist(nodes_coord, metric='euclidean'))\n",
    "\n",
    "            if self.num_neighbors == -1:\n",
    "                W = np.ones((self.num_nodes, self.num_nodes))\n",
    "            else:\n",
    "                W = np.zeros((self.num_nodes, self.num_nodes))\n",
    "                knns = np.argpartition(W_val, kth=self.num_neighbors, axis=-1)[:, self.num_neighbors::-1]\n",
    "                for idx in range(self.num_nodes):\n",
    "                    W[idx][knns[idx]] = 1\n",
    "            np.fill_diagonal(W, 2)\n",
    "\n",
    "            tour_nodes = [int(node) - 1 for node in line[line.index('output') + 1:-1]][:-1]\n",
    "\n",
    "            tour_len = 0\n",
    "            nodes_target = np.zeros(self.num_nodes)\n",
    "            edges_target = np.zeros((self.num_nodes, self.num_nodes))\n",
    "            for idx in range(len(tour_nodes) - 1):\n",
    "                i = tour_nodes[idx]\n",
    "                j = tour_nodes[idx + 1]\n",
    "                nodes_target[i] = idx\n",
    "                edges_target[i][j] = 1\n",
    "                edges_target[j][i] = 1\n",
    "                tour_len += W_val[i][j]\n",
    "\n",
    "            nodes_target[j] = len(tour_nodes) - 1\n",
    "            edges_target[j][tour_nodes[0]] = 1\n",
    "            edges_target[tour_nodes[0]][j] = 1\n",
    "            tour_len += W_val[j][tour_nodes[0]]\n",
    "\n",
    "            batch_edges.append(W)\n",
    "            batch_edges_values.append(W_val)\n",
    "            batch_edges_target.append(edges_target)\n",
    "            batch_nodes.append(nodes)\n",
    "            batch_nodes_target.append(nodes_target)\n",
    "            batch_nodes_coord.append(nodes_coord)\n",
    "            batch_tour_nodes.append(tour_nodes)\n",
    "            batch_tour_len.append(tour_len)\n",
    "\n",
    "        batch = DotDict()\n",
    "        batch.edges = np.stack(batch_edges, axis=0)\n",
    "        batch.edges_values = np.stack(batch_edges_values, axis=0)\n",
    "        batch.edges_target = np.stack(batch_edges_target, axis=0)\n",
    "        batch.nodes = np.stack(batch_nodes, axis=0)\n",
    "        batch.nodes_target = np.stack(batch_nodes_target, axis=0)\n",
    "        batch.nodes_coord = np.stack(batch_nodes_coord, axis=0)\n",
    "        batch.tour_nodes = np.stack(batch_tour_nodes, axis=0)\n",
    "        batch.tour_len = np.stack(batch_tour_len, axis=0)\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Graph Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tour_nodes_to_W(nodes):\n",
    "    \"\"\"Convert ordered list of tour nodes to edge adjacency matrix.\"\"\"\n",
    "    W = np.zeros((len(nodes), len(nodes)))\n",
    "    for idx in range(len(nodes) - 1):\n",
    "        i = int(nodes[idx])\n",
    "        j = int(nodes[idx + 1])\n",
    "        W[i][j] = 1\n",
    "        W[j][i] = 1\n",
    "    W[j][int(nodes[0])] = 1\n",
    "    W[int(nodes[0])][j] = 1\n",
    "    return W\n",
    "\n",
    "\n",
    "def tour_nodes_to_tour_len(nodes, W_values):\n",
    "    \"\"\"Calculate tour length from ordered list of tour nodes.\"\"\"\n",
    "    tour_len = 0\n",
    "    for idx in range(len(nodes) - 1):\n",
    "        i = nodes[idx]\n",
    "        j = nodes[idx + 1]\n",
    "        tour_len += W_values[i][j]\n",
    "    tour_len += W_values[j][nodes[0]]\n",
    "    return tour_len\n",
    "\n",
    "\n",
    "def W_to_tour_len(W, W_values):\n",
    "    \"\"\"Calculate tour length from edge adjacency matrix.\"\"\"\n",
    "    tour_len = 0\n",
    "    for i in range(W.shape[0]):\n",
    "        for j in range(W.shape[1]):\n",
    "            if W[i][j] == 1:\n",
    "                tour_len += W_values[i][j]\n",
    "    tour_len /= 2\n",
    "    return tour_len\n",
    "\n",
    "\n",
    "def is_valid_tour(nodes, num_nodes):\n",
    "    \"\"\"Sanity check: tour visits all nodes given.\"\"\"\n",
    "    return sorted(nodes) == [i for i in range(num_nodes)]\n",
    "\n",
    "\n",
    "def mean_tour_len_edges(x_edges_values, y_pred_edges):\n",
    "    \"\"\"Computes mean tour length for given batch prediction as edge adjacency matrices.\"\"\"\n",
    "    y = F.softmax(y_pred_edges, dim=3)\n",
    "    y = y.argmax(dim=3)\n",
    "    tour_lens = (y.float() * x_edges_values.float()).sum(dim=1).sum(dim=1) / 2\n",
    "    mean_tour_len = tour_lens.sum().to(dtype=torch.float).item() / tour_lens.numel()\n",
    "    return mean_tour_len\n",
    "\n",
    "\n",
    "def mean_tour_len_nodes(x_edges_values, bs_nodes):\n",
    "    \"\"\"Computes mean tour length for given batch prediction as node ordering after beamsearch.\"\"\"\n",
    "    y = bs_nodes.cpu().numpy()\n",
    "    W_val = x_edges_values.cpu().numpy()\n",
    "    running_tour_len = 0\n",
    "    for batch_idx in range(y.shape[0]):\n",
    "        for y_idx in range(y[batch_idx].shape[0] - 1):\n",
    "            i = y[batch_idx][y_idx]\n",
    "            j = y[batch_idx][y_idx + 1]\n",
    "            running_tour_len += W_val[batch_idx][i][j]\n",
    "        running_tour_len += W_val[batch_idx][j][0]\n",
    "    return running_tour_len / y.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. GCN Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNormNode(nn.Module):\n",
    "    \"\"\"Batch normalization for node features.\"\"\"\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(BatchNormNode, self).__init__()\n",
    "        self.batch_norm = nn.BatchNorm1d(hidden_dim, track_running_stats=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_trans = x.transpose(1, 2).contiguous()\n",
    "        x_trans_bn = self.batch_norm(x_trans)\n",
    "        x_bn = x_trans_bn.transpose(1, 2).contiguous()\n",
    "        return x_bn\n",
    "\n",
    "\n",
    "class BatchNormEdge(nn.Module):\n",
    "    \"\"\"Batch normalization for edge features.\"\"\"\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(BatchNormEdge, self).__init__()\n",
    "        self.batch_norm = nn.BatchNorm2d(hidden_dim, track_running_stats=False)\n",
    "\n",
    "    def forward(self, e):\n",
    "        e_trans = e.permute(0, 3, 1, 2).contiguous()\n",
    "        e_trans_bn = self.batch_norm(e_trans)\n",
    "        e_bn = e_trans_bn.permute(0, 2, 3, 1).contiguous()\n",
    "        return e_bn\n",
    "\n",
    "\n",
    "class NodeFeatures(nn.Module):\n",
    "    \"\"\"Convnet features for nodes.\n",
    "    Using `mean` aggregation: x_i = U*x_i + ( sum_j [ gate_ij * (V*x_j) ] / sum_j [ gate_ij] )\n",
    "    Using `sum` aggregation:  x_i = U*x_i + sum_j [ gate_ij * (V*x_j) ]\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim, aggregation=\"mean\"):\n",
    "        super(NodeFeatures, self).__init__()\n",
    "        self.aggregation = aggregation\n",
    "        self.U = nn.Linear(hidden_dim, hidden_dim, True)\n",
    "        self.V = nn.Linear(hidden_dim, hidden_dim, True)\n",
    "\n",
    "    def forward(self, x, edge_gate):\n",
    "        Ux = self.U(x)\n",
    "        Vx = self.V(x)\n",
    "        Vx = Vx.unsqueeze(1)\n",
    "        gateVx = edge_gate * Vx\n",
    "        if self.aggregation == \"mean\":\n",
    "            x_new = Ux + torch.sum(gateVx, dim=2) / (1e-20 + torch.sum(edge_gate, dim=2))\n",
    "        elif self.aggregation == \"sum\":\n",
    "            x_new = Ux + torch.sum(gateVx, dim=2)\n",
    "        return x_new\n",
    "\n",
    "\n",
    "class EdgeFeatures(nn.Module):\n",
    "    \"\"\"Convnet features for edges: e_ij = U*e_ij + V*(x_i + x_j)\"\"\"\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(EdgeFeatures, self).__init__()\n",
    "        self.U = nn.Linear(hidden_dim, hidden_dim, True)\n",
    "        self.V = nn.Linear(hidden_dim, hidden_dim, True)\n",
    "\n",
    "    def forward(self, x, e):\n",
    "        Ue = self.U(e)\n",
    "        Vx = self.V(x)\n",
    "        Wx = Vx.unsqueeze(1)\n",
    "        Vx = Vx.unsqueeze(2)\n",
    "        e_new = Ue + Vx + Wx\n",
    "        return e_new\n",
    "\n",
    "\n",
    "class ResidualGatedGCNLayer(nn.Module):\n",
    "    \"\"\"Convnet layer with gating and residual connection.\"\"\"\n",
    "    def __init__(self, hidden_dim, aggregation=\"sum\"):\n",
    "        super(ResidualGatedGCNLayer, self).__init__()\n",
    "        self.node_feat = NodeFeatures(hidden_dim, aggregation)\n",
    "        self.edge_feat = EdgeFeatures(hidden_dim)\n",
    "        self.bn_node = BatchNormNode(hidden_dim)\n",
    "        self.bn_edge = BatchNormEdge(hidden_dim)\n",
    "\n",
    "    def forward(self, x, e):\n",
    "        e_in = e\n",
    "        x_in = x\n",
    "        e_tmp = self.edge_feat(x_in, e_in)\n",
    "        edge_gate = torch.sigmoid(e_tmp)\n",
    "        x_tmp = self.node_feat(x_in, edge_gate)\n",
    "        e_tmp = self.bn_edge(e_tmp).contiguous()\n",
    "        x_tmp = self.bn_node(x_tmp).contiguous()\n",
    "        e = F.relu(e_tmp)\n",
    "        x = F.relu(x_tmp)\n",
    "        x_new = x_in + x\n",
    "        e_new = e_in + e\n",
    "        return x_new, e_new\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"Multi-layer Perceptron for output prediction.\"\"\"\n",
    "    def __init__(self, hidden_dim, output_dim, L=2):\n",
    "        super(MLP, self).__init__()\n",
    "        self.L = L\n",
    "        U = []\n",
    "        for layer in range(self.L - 1):\n",
    "            U.append(nn.Linear(hidden_dim, hidden_dim, True))\n",
    "        self.U = nn.ModuleList(U)\n",
    "        self.V = nn.Linear(hidden_dim, output_dim, True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        Ux = x\n",
    "        for U_i in self.U:\n",
    "            Ux = U_i(Ux)\n",
    "            Ux = F.relu(Ux)\n",
    "        y = self.V(Ux)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Loss Functions & Beam Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== LOSS FUNCTIONS =====================\n",
    "\n",
    "def loss_edges(y_pred_edges, y_edges, edge_cw):\n",
    "    \"\"\"Loss function for edge predictions.\"\"\"\n",
    "    y = F.log_softmax(y_pred_edges, dim=3)\n",
    "    y = y.permute(0, 3, 1, 2).contiguous()\n",
    "    loss = nn.NLLLoss(edge_cw)(y, y_edges)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def edge_error(y_pred, y_target, x_edges):\n",
    "    \"\"\"Computes edge error metrics for given batch prediction and targets.\"\"\"\n",
    "    y = F.softmax(y_pred, dim=3)\n",
    "    y = y.argmax(dim=3)\n",
    "\n",
    "    mask_no_edges = x_edges.long()\n",
    "    err_edges, _ = _edge_error(y, y_target, mask_no_edges)\n",
    "\n",
    "    mask_no_tour = y_target\n",
    "    err_tour, err_idx_tour = _edge_error(y, y_target, mask_no_tour)\n",
    "\n",
    "    mask_no_tsp = ((y_target + y) > 0).long()\n",
    "    err_tsp, err_idx_tsp = _edge_error(y, y_target, mask_no_tsp)\n",
    "\n",
    "    return 100 * err_edges, 100 * err_tour, 100 * err_tsp, err_idx_tour, err_idx_tsp\n",
    "\n",
    "\n",
    "def _edge_error(y, y_target, mask):\n",
    "    \"\"\"Helper method to compute edge errors.\"\"\"\n",
    "    acc = (y == y_target).long()\n",
    "    acc = (acc * mask)\n",
    "    acc = acc.sum(dim=1).sum(dim=1).to(dtype=torch.float) / mask.sum(dim=1).sum(dim=1).to(dtype=torch.float)\n",
    "    err_idx = (acc < 1.0)\n",
    "    acc = acc.sum().to(dtype=torch.float).item() / acc.numel()\n",
    "    err = 1.0 - acc\n",
    "    return err, err_idx\n",
    "\n",
    "\n",
    "# ===================== BEAM SEARCH =====================\n",
    "\n",
    "class Beamsearch(object):\n",
    "    \"\"\"Class for managing internals of beamsearch procedure.\"\"\"\n",
    "\n",
    "    def __init__(self, beam_size, batch_size, num_nodes,\n",
    "                 dtypeFloat=torch.FloatTensor, dtypeLong=torch.LongTensor,\n",
    "                 probs_type='raw', random_start=False):\n",
    "        self.batch_size = batch_size\n",
    "        self.beam_size = beam_size\n",
    "        self.num_nodes = num_nodes\n",
    "        self.probs_type = probs_type\n",
    "        self.dtypeFloat = dtypeFloat\n",
    "        self.dtypeLong = dtypeLong\n",
    "        self.start_nodes = torch.zeros(batch_size, beam_size).type(self.dtypeLong)\n",
    "        if random_start:\n",
    "            self.start_nodes = torch.randint(0, num_nodes, (batch_size, beam_size)).type(self.dtypeLong)\n",
    "        self.mask = torch.ones(batch_size, beam_size, num_nodes).type(self.dtypeFloat)\n",
    "        self.update_mask(self.start_nodes)\n",
    "        self.scores = torch.zeros(batch_size, beam_size).type(self.dtypeFloat)\n",
    "        self.all_scores = []\n",
    "        self.prev_Ks = []\n",
    "        self.next_nodes = [self.start_nodes]\n",
    "\n",
    "    def get_current_state(self):\n",
    "        current_state = (self.next_nodes[-1].unsqueeze(2)\n",
    "                         .expand(self.batch_size, self.beam_size, self.num_nodes))\n",
    "        return current_state\n",
    "\n",
    "    def get_current_origin(self):\n",
    "        return self.prev_Ks[-1]\n",
    "\n",
    "    def advance(self, trans_probs):\n",
    "        if len(self.prev_Ks) > 0:\n",
    "            if self.probs_type == 'raw':\n",
    "                beam_lk = trans_probs * self.scores.unsqueeze(2).expand_as(trans_probs)\n",
    "            elif self.probs_type == 'logits':\n",
    "                beam_lk = trans_probs + self.scores.unsqueeze(2).expand_as(trans_probs)\n",
    "        else:\n",
    "            beam_lk = trans_probs\n",
    "            if self.probs_type == 'raw':\n",
    "                beam_lk[:, 1:] = torch.zeros(beam_lk[:, 1:].size()).type(self.dtypeFloat)\n",
    "            elif self.probs_type == 'logits':\n",
    "                beam_lk[:, 1:] = -1e20 * torch.ones(beam_lk[:, 1:].size()).type(self.dtypeFloat)\n",
    "        beam_lk = beam_lk * self.mask\n",
    "        beam_lk = beam_lk.view(self.batch_size, -1)\n",
    "        bestScores, bestScoresId = beam_lk.topk(self.beam_size, 1, True, True)\n",
    "        self.scores = bestScores\n",
    "        prev_k = bestScoresId // self.num_nodes\n",
    "        self.prev_Ks.append(prev_k)\n",
    "        new_nodes = bestScoresId - prev_k * self.num_nodes\n",
    "        self.next_nodes.append(new_nodes)\n",
    "        perm_mask = prev_k.unsqueeze(2).expand_as(self.mask)\n",
    "        self.mask = self.mask.gather(1, perm_mask)\n",
    "        self.update_mask(new_nodes)\n",
    "\n",
    "    def update_mask(self, new_nodes):\n",
    "        arr = (torch.arange(0, self.num_nodes).unsqueeze(0).unsqueeze(1)\n",
    "               .expand_as(self.mask).type(self.dtypeLong))\n",
    "        new_nodes = new_nodes.unsqueeze(2).expand_as(self.mask)\n",
    "        update_mask = 1 - torch.eq(arr, new_nodes).type(self.dtypeFloat)\n",
    "        self.mask = self.mask * update_mask\n",
    "        if self.probs_type == 'logits':\n",
    "            self.mask[self.mask == 0] = 1e20\n",
    "\n",
    "    def sort_best(self):\n",
    "        return torch.sort(self.scores, 0, True)\n",
    "\n",
    "    def get_best(self):\n",
    "        scores, ids = self.sort_best()\n",
    "        return scores[1], ids[1]\n",
    "\n",
    "    def get_hypothesis(self, k):\n",
    "        assert self.num_nodes == len(self.prev_Ks) + 1\n",
    "        hyp = -1 * torch.ones(self.batch_size, self.num_nodes).type(self.dtypeLong)\n",
    "        for j in range(len(self.prev_Ks) - 1, -2, -1):\n",
    "            hyp[:, j + 1] = self.next_nodes[j + 1].gather(1, k).view(1, self.batch_size)\n",
    "            k = self.prev_Ks[j].gather(1, k)\n",
    "        return hyp\n",
    "\n",
    "\n",
    "def beamsearch_tour_nodes(y_pred_edges, beam_size, batch_size, num_nodes,\n",
    "                          dtypeFloat, dtypeLong, probs_type='raw', random_start=False):\n",
    "    \"\"\"Performs beamsearch on edge prediction matrices and returns possible TSP tours.\"\"\"\n",
    "    if probs_type == 'raw':\n",
    "        y = F.softmax(y_pred_edges, dim=3)[:, :, :, 1]\n",
    "    elif probs_type == 'logits':\n",
    "        y = F.log_softmax(y_pred_edges, dim=3)[:, :, :, 1]\n",
    "        y[y == 0] = -1e-20\n",
    "    beamsearch = Beamsearch(beam_size, batch_size, num_nodes, dtypeFloat, dtypeLong, probs_type, random_start)\n",
    "    trans_probs = y.gather(1, beamsearch.get_current_state())\n",
    "    for step in range(num_nodes - 1):\n",
    "        beamsearch.advance(trans_probs)\n",
    "        trans_probs = y.gather(1, beamsearch.get_current_state())\n",
    "    ends = torch.zeros(batch_size, 1).type(dtypeLong)\n",
    "    return beamsearch.get_hypothesis(ends)\n",
    "\n",
    "\n",
    "def beamsearch_tour_nodes_shortest(y_pred_edges, x_edges_values, beam_size, batch_size, num_nodes,\n",
    "                                   dtypeFloat, dtypeLong, probs_type='raw', random_start=False):\n",
    "    \"\"\"Beamsearch with shortest tour heuristic.\"\"\"\n",
    "    if probs_type == 'raw':\n",
    "        y = F.softmax(y_pred_edges, dim=3)[:, :, :, 1]\n",
    "    elif probs_type == 'logits':\n",
    "        y = F.log_softmax(y_pred_edges, dim=3)[:, :, :, 1]\n",
    "        y[y == 0] = -1e-20\n",
    "    beamsearch = Beamsearch(beam_size, batch_size, num_nodes, dtypeFloat, dtypeLong, probs_type, random_start)\n",
    "    trans_probs = y.gather(1, beamsearch.get_current_state())\n",
    "    for step in range(num_nodes - 1):\n",
    "        beamsearch.advance(trans_probs)\n",
    "        trans_probs = y.gather(1, beamsearch.get_current_state())\n",
    "    ends = torch.zeros(batch_size, 1).type(dtypeLong)\n",
    "    shortest_tours = beamsearch.get_hypothesis(ends)\n",
    "    shortest_lens = [1e6] * len(shortest_tours)\n",
    "    for idx in range(len(shortest_tours)):\n",
    "        shortest_lens[idx] = tour_nodes_to_tour_len(shortest_tours[idx].cpu().numpy(),\n",
    "                                                    x_edges_values[idx].cpu().numpy())\n",
    "    for pos in range(1, beam_size):\n",
    "        ends = pos * torch.ones(batch_size, 1).type(dtypeLong)\n",
    "        hyp_tours = beamsearch.get_hypothesis(ends)\n",
    "        for idx in range(len(hyp_tours)):\n",
    "            hyp_nodes = hyp_tours[idx].cpu().numpy()\n",
    "            hyp_len = tour_nodes_to_tour_len(hyp_nodes, x_edges_values[idx].cpu().numpy())\n",
    "            if hyp_len < shortest_lens[idx] and is_valid_tour(hyp_nodes, num_nodes):\n",
    "                shortest_tours[idx] = hyp_tours[idx]\n",
    "                shortest_lens[idx] = hyp_len\n",
    "    return shortest_tours\n",
    "\n",
    "\n",
    "def update_learning_rate(optimizer, lr):\n",
    "    \"\"\"Updates learning rate for given optimizer.\"\"\"\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. GCN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualGatedGCNModel(nn.Module):\n",
    "    \"\"\"Residual Gated GCN Model for outputting predictions as edge adjacency matrices.\n",
    "    Reference: https://arxiv.org/pdf/1711.07553v2.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, dtypeFloat, dtypeLong):\n",
    "        super(ResidualGatedGCNModel, self).__init__()\n",
    "        self.dtypeFloat = dtypeFloat\n",
    "        self.dtypeLong = dtypeLong\n",
    "        self.num_nodes = config.num_nodes\n",
    "        self.node_dim = config.node_dim\n",
    "        self.voc_nodes_in = config['voc_nodes_in']\n",
    "        self.voc_nodes_out = config['num_nodes']\n",
    "        self.voc_edges_in = config['voc_edges_in']\n",
    "        self.voc_edges_out = config['voc_edges_out']\n",
    "        self.hidden_dim = config['hidden_dim']\n",
    "        self.num_layers = config['num_layers']\n",
    "        self.mlp_layers = config['mlp_layers']\n",
    "        self.aggregation = config['aggregation']\n",
    "        # Embeddings\n",
    "        self.nodes_coord_embedding = nn.Linear(self.node_dim, self.hidden_dim, bias=False)\n",
    "        self.edges_values_embedding = nn.Linear(1, self.hidden_dim // 2, bias=False)\n",
    "        self.edges_embedding = nn.Embedding(self.voc_edges_in, self.hidden_dim // 2)\n",
    "        # GCN Layers\n",
    "        gcn_layers = []\n",
    "        for layer in range(self.num_layers):\n",
    "            gcn_layers.append(ResidualGatedGCNLayer(self.hidden_dim, self.aggregation))\n",
    "        self.gcn_layers = nn.ModuleList(gcn_layers)\n",
    "        # MLP classifier\n",
    "        self.mlp_edges = MLP(self.hidden_dim, self.voc_edges_out, self.mlp_layers)\n",
    "\n",
    "    def forward(self, x_edges, x_edges_values, x_nodes, x_nodes_coord, y_edges, edge_cw):\n",
    "        # Embeddings\n",
    "        x = self.nodes_coord_embedding(x_nodes_coord)\n",
    "        e_vals = self.edges_values_embedding(x_edges_values.unsqueeze(3))\n",
    "        e_tags = self.edges_embedding(x_edges)\n",
    "        e = torch.cat((e_vals, e_tags), dim=3)\n",
    "        # GCN layers\n",
    "        for layer in range(self.num_layers):\n",
    "            x, e = self.gcn_layers[layer](x, e)\n",
    "        # MLP classifier\n",
    "        y_pred_edges = self.mlp_edges(e)\n",
    "        # Loss\n",
    "        edge_cw = torch.Tensor(edge_cw).type(self.dtypeFloat)\n",
    "        loss = loss_edges(y_pred_edges, y_edges, edge_cw)\n",
    "        return y_pred_edges, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Plot Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tsp(p, x_coord, W, W_val, W_target, title=\"default\"):\n",
    "    \"\"\"Plot TSP tours.\"\"\"\n",
    "    def _edges_to_node_pairs(W):\n",
    "        pairs = []\n",
    "        for r in range(len(W)):\n",
    "            for c in range(len(W)):\n",
    "                if W[r][c] == 1:\n",
    "                    pairs.append((r, c))\n",
    "        return pairs\n",
    "\n",
    "    G = nx.from_numpy_array(W_val)\n",
    "    pos = dict(zip(range(len(x_coord)), x_coord.tolist()))\n",
    "    adj_pairs = _edges_to_node_pairs(W)\n",
    "    target_pairs = _edges_to_node_pairs(W_target)\n",
    "    colors = ['g'] + ['b'] * (len(x_coord) - 1)\n",
    "    nx.draw_networkx_nodes(G, pos, node_color=colors, node_size=50)\n",
    "    nx.draw_networkx_edges(G, pos, edgelist=adj_pairs, alpha=0.3, width=0.5)\n",
    "    nx.draw_networkx_edges(G, pos, edgelist=target_pairs, alpha=1, width=1, edge_color='r')\n",
    "    p.set_title(title)\n",
    "    return p\n",
    "\n",
    "\n",
    "def plot_tsp_heatmap(p, x_coord, W_val, W_pred, title=\"default\"):\n",
    "    \"\"\"Plot predicted TSP tours with edge strength as confidence.\"\"\"\n",
    "    def _edges_to_node_pairs(W):\n",
    "        pairs = []\n",
    "        edge_preds = []\n",
    "        for r in range(len(W)):\n",
    "            for c in range(len(W)):\n",
    "                if W[r][c] > 0.25:\n",
    "                    pairs.append((r, c))\n",
    "                    edge_preds.append(W[r][c])\n",
    "        return pairs, edge_preds\n",
    "\n",
    "    G = nx.from_numpy_array(W_val)\n",
    "    pos = dict(zip(range(len(x_coord)), x_coord.tolist()))\n",
    "    node_pairs, edge_color = _edges_to_node_pairs(W_pred)\n",
    "    node_color = ['g'] + ['b'] * (len(x_coord) - 1)\n",
    "    nx.draw_networkx_nodes(G, pos, node_color=node_color, node_size=50)\n",
    "    nx.draw_networkx_edges(G, pos, edgelist=node_pairs, edge_color=edge_color, edge_cmap=plt.cm.Reds, width=0.75)\n",
    "    p.set_title(title)\n",
    "    return p\n",
    "\n",
    "\n",
    "def plot_predictions_beamsearch(x_nodes_coord, x_edges, x_edges_values, y_edges, y_pred_edges, bs_nodes, num_plots=3):\n",
    "    \"\"\"Plots groundtruth TSP tour vs. predicted tours (with beamsearch).\"\"\"\n",
    "    y = F.softmax(y_pred_edges, dim=3)\n",
    "    y_bins = y.argmax(dim=3)\n",
    "    y_probs = y[:, :, :, 1]\n",
    "    for f_idx, idx in enumerate(np.random.choice(len(y), min(num_plots, len(y)), replace=False)):\n",
    "        f = plt.figure(f_idx, figsize=(15, 5))\n",
    "        x_coord = x_nodes_coord[idx].cpu().numpy()\n",
    "        W = x_edges[idx].cpu().numpy()\n",
    "        W_val = x_edges_values[idx].cpu().numpy()\n",
    "        W_target = y_edges[idx].cpu().numpy()\n",
    "        W_sol_probs = y_probs[idx].cpu().numpy()\n",
    "        W_bs = tour_nodes_to_W(bs_nodes[idx].cpu().numpy())\n",
    "        plt1 = f.add_subplot(131)\n",
    "        plot_tsp(plt1, x_coord, W, W_val, W_target, 'GT: {:.3f}'.format(W_to_tour_len(W_target, W_val)))\n",
    "        plt2 = f.add_subplot(132)\n",
    "        plot_tsp_heatmap(plt2, x_coord, W_val, W_sol_probs, 'Heatmap')\n",
    "        plt3 = f.add_subplot(133)\n",
    "        plot_tsp(plt3, x_coord, W, W_val, W_bs, 'BS: {:.3f}'.format(W_to_tour_len(W_bs, W_val)))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Configuration de l'exp√©rience\n",
    "\n",
    "**Modifiez les param√®tres ci-dessous selon vos besoins** (TSP10, TSP20, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PARAMETRES A MODIFIER SELON VOTRE EXPERIENCE\n",
    "# ============================================================\n",
    "# Choisir: 'tsp10' ou 'tsp20'\n",
    "TSP_SIZE = 'tsp10'\n",
    "\n",
    "# Chemin vers les donn√©es dans Kaggle (adaptez si n√©cessaire)\n",
    "# Les donn√©es sont typiquement dans /kaggle/input/<nom-dataset>/\n",
    "DATA_DIR = '/kaggle/input'  # Dossier racine des donn√©es\n",
    "\n",
    "# Trouver automatiquement les fichiers de donn√©es\n",
    "print(\"Fichiers disponibles dans /kaggle/input:\")\n",
    "for root, dirs, files in os.walk('/kaggle/input'):\n",
    "    for f in files:\n",
    "        if 'tsp' in f.lower():\n",
    "            print(f\"  {os.path.join(root, f)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DETECTION AUTOMATIQUE DES CHEMINS DES DONNEES\n",
    "# ============================================================\n",
    "def find_data_file(pattern):\n",
    "    \"\"\"Cherche un fichier de donn√©es dans /kaggle/input/\"\"\"\n",
    "    matches = glob.glob(f'/kaggle/input/**/{pattern}', recursive=True)\n",
    "    if matches:\n",
    "        return matches[0]\n",
    "    # Essayer aussi sans sous-dossier\n",
    "    matches = glob.glob(f'/kaggle/input/{pattern}')\n",
    "    if matches:\n",
    "        return matches[0]\n",
    "    raise FileNotFoundError(f\"Fichier '{pattern}' non trouv√© dans /kaggle/input/\")\n",
    "\n",
    "if TSP_SIZE == 'tsp10':\n",
    "    config_dict = {\n",
    "        \"expt_name\": \"tsp10\",\n",
    "        \"gpu_id\": \"0,1\",\n",
    "        \"train_filepath\": find_data_file(\"tsp10_train_concorde.txt\"),\n",
    "        \"val_filepath\": find_data_file(\"tsp10_val_concorde.txt\"),\n",
    "        \"test_filepath\": find_data_file(\"tsp10_test_concorde.txt\"),\n",
    "        \"num_nodes\": 10,\n",
    "        \"num_neighbors\": -1,\n",
    "        \"node_dim\": 2,\n",
    "        \"voc_nodes_in\": 2,\n",
    "        \"voc_nodes_out\": 2,\n",
    "        \"voc_edges_in\": 3,\n",
    "        \"voc_edges_out\": 2,\n",
    "        \"beam_size\": 1280,\n",
    "        \"hidden_dim\": 300,\n",
    "        \"num_layers\": 30,\n",
    "        \"mlp_layers\": 3,\n",
    "        \"aggregation\": \"mean\",\n",
    "        \"max_epochs\": 1500,\n",
    "        \"val_every\": 5,\n",
    "        \"test_every\": 100,\n",
    "        \"batch_size\": 20,\n",
    "        \"batches_per_epoch\": 500,\n",
    "        \"accumulation_steps\": 1,\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"decay_rate\": 1.01\n",
    "    }\n",
    "elif TSP_SIZE == 'tsp20':\n",
    "    config_dict = {\n",
    "        \"expt_name\": \"tsp20\",\n",
    "        \"gpu_id\": \"0,1\",\n",
    "        \"train_filepath\": find_data_file(\"tsp20_train_concorde.txt\"),\n",
    "        \"val_filepath\": find_data_file(\"tsp20_val_concorde.txt\"),\n",
    "        \"test_filepath\": find_data_file(\"tsp20_test_concorde.txt\"),\n",
    "        \"num_nodes\": 20,\n",
    "        \"num_neighbors\": -1,\n",
    "        \"node_dim\": 2,\n",
    "        \"voc_nodes_in\": 2,\n",
    "        \"voc_nodes_out\": 2,\n",
    "        \"voc_edges_in\": 3,\n",
    "        \"voc_edges_out\": 2,\n",
    "        \"beam_size\": 1280,\n",
    "        \"hidden_dim\": 300,\n",
    "        \"num_layers\": 30,\n",
    "        \"mlp_layers\": 3,\n",
    "        \"aggregation\": \"mean\",\n",
    "        \"max_epochs\": 1500,\n",
    "        \"val_every\": 5,\n",
    "        \"test_every\": 100,\n",
    "        \"batch_size\": 20,\n",
    "        \"batches_per_epoch\": 500,\n",
    "        \"accumulation_steps\": 1,\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"decay_rate\": 1.01\n",
    "    }\n",
    "\n",
    "config = Settings(config_dict)\n",
    "print(f\"Configuration charg√©e pour {TSP_SIZE}:\")\n",
    "for k, v in config.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Configuration GPU (Double GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration pour utiliser les 2 GPU Kaggle\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(config.gpu_id)  # \"0,1\"\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(f\"CUDA disponible - {num_gpus} GPU(s) d√©tect√©(s)\")\n",
    "    for i in range(num_gpus):\n",
    "        print(f\"  GPU {i}: {torch.cuda.get_device_name(i)} - \"\n",
    "              f\"{torch.cuda.get_device_properties(i).total_memory / 1e9:.1f} GB\")\n",
    "    dtypeFloat = torch.cuda.FloatTensor\n",
    "    dtypeLong = torch.cuda.LongTensor\n",
    "    torch.cuda.manual_seed(1)\n",
    "else:\n",
    "    print(\"CUDA non disponible, utilisation du CPU\")\n",
    "    dtypeFloat = torch.FloatTensor\n",
    "    dtypeLong = torch.LongTensor\n",
    "    torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Exploration des donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = config.num_nodes\n",
    "num_neighbors = config.num_neighbors\n",
    "batch_size = config.batch_size\n",
    "train_filepath = config.train_filepath\n",
    "\n",
    "dataset = GoogleTSPReader(num_nodes, num_neighbors, batch_size, train_filepath)\n",
    "print(f\"Nombre de batches de taille {batch_size}: {dataset.max_iter}\")\n",
    "\n",
    "t = time.time()\n",
    "batch = next(iter(dataset))\n",
    "print(f\"G√©n√©ration du batch: {time.time() - t:.3f} sec\")\n",
    "\n",
    "print(f\"\\nFormes des tenseurs:\")\n",
    "print(f\"  edges:         {batch.edges.shape}\")\n",
    "print(f\"  edges_values:  {batch.edges_values.shape}\")\n",
    "print(f\"  edges_targets: {batch.edges_target.shape}\")\n",
    "print(f\"  nodes:         {batch.nodes.shape}\")\n",
    "print(f\"  nodes_target:  {batch.nodes_target.shape}\")\n",
    "print(f\"  nodes_coord:   {batch.nodes_coord.shape}\")\n",
    "print(f\"  tour_nodes:    {batch.tour_nodes.shape}\")\n",
    "print(f\"  tour_len:      {batch.tour_len.shape}\")\n",
    "\n",
    "# Visualiser un exemple\n",
    "idx = 0\n",
    "f = plt.figure(figsize=(5, 5))\n",
    "a = f.add_subplot(111)\n",
    "plot_tsp(a, batch.nodes_coord[idx], batch.edges[idx], batch.edges_values[idx], batch.edges_target[idx])\n",
    "plt.title(f\"Exemple TSP{num_nodes} - Tour length: {batch.tour_len[idx]:.3f}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Training & Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(net, optimizer, config, master_bar):\n",
    "    \"\"\"Train for one epoch.\"\"\"\n",
    "    net.train()\n",
    "\n",
    "    # Afficher le device utilis√©\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"üöÄ Training sur {torch.cuda.device_count()} GPU(s): {[torch.cuda.get_device_name(i) for i in range(torch.cuda.device_count())]}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Training sur CPU\")\n",
    "\n",
    "    num_nodes = config.num_nodes\n",
    "    num_neighbors = config.num_neighbors\n",
    "    batch_size = config.batch_size\n",
    "    batches_per_epoch = config.batches_per_epoch\n",
    "    accumulation_steps = config.accumulation_steps\n",
    "    train_filepath = config.train_filepath\n",
    "\n",
    "    dataset = GoogleTSPReader(num_nodes, num_neighbors, batch_size, train_filepath)\n",
    "    if batches_per_epoch != -1:\n",
    "        batches_per_epoch = min(batches_per_epoch, dataset.max_iter)\n",
    "    else:\n",
    "        batches_per_epoch = dataset.max_iter\n",
    "\n",
    "    dataset = iter(dataset)\n",
    "    edge_cw = None\n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_pred_tour_len = 0.0\n",
    "    running_gt_tour_len = 0.0\n",
    "    running_nb_data = 0\n",
    "    running_nb_batch = 0\n",
    "\n",
    "    start_epoch = time.time()\n",
    "    for batch_num in progress_bar(range(batches_per_epoch), parent=master_bar):\n",
    "        try:\n",
    "            batch = next(dataset)\n",
    "        except StopIteration:\n",
    "            break\n",
    "\n",
    "        # Cr√©er les tenseurs et les d√©placer vers GPU si disponible\n",
    "        x_edges = torch.LongTensor(batch.edges).type(dtypeLong)\n",
    "        x_edges_values = torch.FloatTensor(batch.edges_values).type(dtypeFloat)\n",
    "        x_nodes = torch.LongTensor(batch.nodes).type(dtypeLong)\n",
    "        x_nodes_coord = torch.FloatTensor(batch.nodes_coord).type(dtypeFloat)\n",
    "        y_edges = torch.LongTensor(batch.edges_target).type(dtypeLong)\n",
    "        y_nodes = torch.LongTensor(batch.nodes_target).type(dtypeLong)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            x_edges = x_edges.cuda()\n",
    "            x_edges_values = x_edges_values.cuda()\n",
    "            x_nodes = x_nodes.cuda()\n",
    "            x_nodes_coord = x_nodes_coord.cuda()\n",
    "            y_edges = y_edges.cuda()\n",
    "            y_nodes = y_nodes.cuda()\n",
    "\n",
    "        if type(edge_cw) != torch.Tensor:\n",
    "            edge_labels = y_edges.cpu().numpy().flatten()\n",
    "            edge_cw = compute_class_weight(\"balanced\", classes=np.unique(edge_labels), y=edge_labels)\n",
    "\n",
    "        y_preds, loss = net.forward(x_edges, x_edges_values, x_nodes, x_nodes_coord, y_edges, edge_cw)\n",
    "        loss = loss.mean()\n",
    "        loss = loss / accumulation_steps\n",
    "        loss.backward()\n",
    "\n",
    "        if (batch_num + 1) % accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        pred_tour_len = mean_tour_len_edges(x_edges_values, y_preds)\n",
    "        gt_tour_len = np.mean(batch.tour_len)\n",
    "\n",
    "        running_nb_data += batch_size\n",
    "        running_loss += batch_size * loss.data.item() * accumulation_steps\n",
    "        running_pred_tour_len += batch_size * pred_tour_len\n",
    "        running_gt_tour_len += batch_size * gt_tour_len\n",
    "        running_nb_batch += 1\n",
    "\n",
    "        result = ('loss:{loss:.4f} pred:{pred:.3f} gt:{gt:.3f}'.format(\n",
    "            loss=running_loss / running_nb_data,\n",
    "            pred=running_pred_tour_len / running_nb_data,\n",
    "            gt=running_gt_tour_len / running_nb_data))\n",
    "        master_bar.child.comment = result\n",
    "\n",
    "    loss = running_loss / running_nb_data\n",
    "    pred_tour_len = running_pred_tour_len / running_nb_data\n",
    "    gt_tour_len = running_gt_tour_len / running_nb_data\n",
    "\n",
    "    return time.time() - start_epoch, loss, 0, 0, 0, pred_tour_len, gt_tour_len\n",
    "\n",
    "\n",
    "def test(net, config, master_bar, mode='test'):\n",
    "    \"\"\"Evaluate on validation or test set.\"\"\"\n",
    "    net.eval()\n",
    "\n",
    "    # Afficher le device utilis√©\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"üöÄ Evaluation ({mode}) sur {torch.cuda.device_count()} GPU(s): {[torch.cuda.get_device_name(i) for i in range(torch.cuda.device_count())]}\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Evaluation ({mode}) sur CPU\")\n",
    "\n",
    "    num_nodes = config.num_nodes\n",
    "    num_neighbors = config.num_neighbors\n",
    "    batch_size = config.batch_size\n",
    "    beam_size = config.beam_size\n",
    "    val_filepath = config.val_filepath\n",
    "    test_filepath = config.test_filepath\n",
    "\n",
    "    if mode == 'val':\n",
    "        dataset = GoogleTSPReader(num_nodes, num_neighbors, batch_size=batch_size, filepath=val_filepath)\n",
    "    elif mode == 'test':\n",
    "        dataset = GoogleTSPReader(num_nodes, num_neighbors, batch_size=batch_size, filepath=test_filepath)\n",
    "    batches_per_epoch = dataset.max_iter\n",
    "\n",
    "    dataset = iter(dataset)\n",
    "    edge_cw = None\n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_pred_tour_len = 0.0\n",
    "    running_gt_tour_len = 0.0\n",
    "    running_nb_data = 0\n",
    "    running_nb_batch = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        start_test = time.time()\n",
    "        for batch_num in progress_bar(range(batches_per_epoch), parent=master_bar):\n",
    "            try:\n",
    "                batch = next(dataset)\n",
    "            except StopIteration:\n",
    "                break\n",
    "\n",
    "            # Cr√©er les tenseurs et les d√©placer vers GPU si disponible\n",
    "            x_edges = torch.LongTensor(batch.edges).type(dtypeLong)\n",
    "            x_edges_values = torch.FloatTensor(batch.edges_values).type(dtypeFloat)\n",
    "            x_nodes = torch.LongTensor(batch.nodes).type(dtypeLong)\n",
    "            x_nodes_coord = torch.FloatTensor(batch.nodes_coord).type(dtypeFloat)\n",
    "            y_edges = torch.LongTensor(batch.edges_target).type(dtypeLong)\n",
    "            y_nodes = torch.LongTensor(batch.nodes_target).type(dtypeLong)\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                x_edges = x_edges.cuda()\n",
    "                x_edges_values = x_edges_values.cuda()\n",
    "                x_nodes = x_nodes.cuda()\n",
    "                x_nodes_coord = x_nodes_coord.cuda()\n",
    "                y_edges = y_edges.cuda()\n",
    "                y_nodes = y_nodes.cuda()\n",
    "\n",
    "            if type(edge_cw) != torch.Tensor:\n",
    "                edge_labels = y_edges.cpu().numpy().flatten()\n",
    "                edge_cw = compute_class_weight(\"balanced\", classes=np.unique(edge_labels), y=edge_labels)\n",
    "\n",
    "            y_preds, loss = net.forward(x_edges, x_edges_values, x_nodes, x_nodes_coord, y_edges, edge_cw)\n",
    "            loss = loss.mean()\n",
    "\n",
    "            if mode == 'val':\n",
    "                bs_nodes = beamsearch_tour_nodes(\n",
    "                    y_preds, beam_size, batch_size, num_nodes, dtypeFloat, dtypeLong, probs_type='logits')\n",
    "            elif mode == 'test':\n",
    "                bs_nodes = beamsearch_tour_nodes_shortest(\n",
    "                    y_preds, x_edges_values, beam_size, batch_size, num_nodes, dtypeFloat, dtypeLong, probs_type='logits')\n",
    "\n",
    "            pred_tour_len = mean_tour_len_nodes(x_edges_values, bs_nodes)\n",
    "            gt_tour_len = np.mean(batch.tour_len)\n",
    "\n",
    "            running_nb_data += batch_size\n",
    "            running_loss += batch_size * loss.data.item()\n",
    "            running_pred_tour_len += batch_size * pred_tour_len\n",
    "            running_gt_tour_len += batch_size * gt_tour_len\n",
    "            running_nb_batch += 1\n",
    "\n",
    "            result = ('loss:{loss:.4f} pred:{pred:.3f} gt:{gt:.3f}'.format(\n",
    "                loss=running_loss / running_nb_data,\n",
    "                pred=running_pred_tour_len / running_nb_data,\n",
    "                gt=running_gt_tour_len / running_nb_data))\n",
    "            master_bar.child.comment = result\n",
    "\n",
    "    loss = running_loss / running_nb_data\n",
    "    pred_tour_len = running_pred_tour_len / running_nb_data\n",
    "    gt_tour_len = running_gt_tour_len / running_nb_data\n",
    "\n",
    "    return time.time() - start_test, loss, 0, 0, 0, pred_tour_len, gt_tour_len\n",
    "\n",
    "\n",
    "def metrics_to_str(epoch, time, learning_rate, loss, err_edges, err_tour, err_tsp, pred_tour_len, gt_tour_len):\n",
    "    result = ('epoch:{epoch:0>2d} '\n",
    "              'time:{time:.1f}h '\n",
    "              'lr:{learning_rate:.2e} '\n",
    "              'loss:{loss:.4f} '\n",
    "              'pred_tour_len:{pred_tour_len:.3f} '\n",
    "              'gt_tour_len:{gt_tour_len:.3f}'.format(\n",
    "                  epoch=epoch, time=time / 3600,\n",
    "                  learning_rate=learning_rate, loss=loss,\n",
    "                  pred_tour_len=pred_tour_len, gt_tour_len=gt_tour_len))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Pipeline d'entra√Ænement complet (Double GPU avec DataParallel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(config):\n",
    "    \"\"\"Full training pipeline with multi-GPU support.\"\"\"\n",
    "    # Instancier le mod√®le\n",
    "    net = ResidualGatedGCNModel(config, dtypeFloat, dtypeLong)\n",
    "\n",
    "    # D√©placer explicitement vers GPU AVANT DataParallel\n",
    "    if torch.cuda.is_available():\n",
    "        net = net.cuda()\n",
    "        net = nn.DataParallel(net)  # DataParallel APR√àS .cuda()\n",
    "        print(f\"Mod√®le distribu√© sur {torch.cuda.device_count()} GPU(s)\")\n",
    "    else:\n",
    "        print(\"Utilisation du CPU\")\n",
    "\n",
    "    print(net)\n",
    "\n",
    "    # Nombre de param√®tres\n",
    "    nb_param = sum(p.numel() for p in net.parameters())\n",
    "    print(f'Nombre de param√®tres: {nb_param:,}')\n",
    "\n",
    "    # Cr√©er le r√©pertoire de logs\n",
    "    log_dir = f\"/kaggle/working/logs/{config.expt_name}/\"\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    json.dump(dict(config), open(f\"{log_dir}/config.json\", \"w\"), indent=4)\n",
    "    writer = SummaryWriter(log_dir)\n",
    "\n",
    "    # Param√®tres d'entra√Ænement\n",
    "    max_epochs = config.max_epochs\n",
    "    val_every = config.val_every\n",
    "    test_every = config.test_every\n",
    "    learning_rate = config.learning_rate\n",
    "    decay_rate = config.decay_rate\n",
    "    val_loss_old = 1e6\n",
    "    best_pred_tour_len = 1e6\n",
    "\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "    print(optimizer)\n",
    "\n",
    "    epoch_bar = master_bar(range(max_epochs))\n",
    "    for epoch in epoch_bar:\n",
    "        writer.add_scalar('learning_rate', learning_rate, epoch)\n",
    "\n",
    "        # Train\n",
    "        train_time, train_loss, _, _, _, train_pred_tour_len, train_gt_tour_len = \\\n",
    "            train_one_epoch(net, optimizer, config, epoch_bar)\n",
    "        epoch_bar.write('t: ' + metrics_to_str(epoch, train_time, learning_rate, train_loss,\n",
    "                                                0, 0, 0, train_pred_tour_len, train_gt_tour_len))\n",
    "        writer.add_scalar('loss/train_loss', train_loss, epoch)\n",
    "        writer.add_scalar('pred_tour_len/train_pred_tour_len', train_pred_tour_len, epoch)\n",
    "        writer.add_scalar('optimality_gap/train_opt_gap', train_pred_tour_len / train_gt_tour_len - 1, epoch)\n",
    "\n",
    "        if epoch % val_every == 0 or epoch == max_epochs - 1:\n",
    "            # Validate\n",
    "            val_time, val_loss, _, _, _, val_pred_tour_len, val_gt_tour_len = \\\n",
    "                test(net, config, epoch_bar, mode='val')\n",
    "            epoch_bar.write('v: ' + metrics_to_str(epoch, val_time, learning_rate, val_loss,\n",
    "                                                    0, 0, 0, val_pred_tour_len, val_gt_tour_len))\n",
    "            writer.add_scalar('loss/val_loss', val_loss, epoch)\n",
    "            writer.add_scalar('pred_tour_len/val_pred_tour_len', val_pred_tour_len, epoch)\n",
    "            writer.add_scalar('optimality_gap/val_opt_gap', val_pred_tour_len / val_gt_tour_len - 1, epoch)\n",
    "\n",
    "            # Sauvegarder le meilleur mod√®le\n",
    "            if val_pred_tour_len < best_pred_tour_len:\n",
    "                best_pred_tour_len = val_pred_tour_len\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': net.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'train_loss': train_loss,\n",
    "                    'val_loss': val_loss,\n",
    "                }, log_dir + \"best_val_checkpoint.tar\")\n",
    "\n",
    "            # Mise √† jour du learning rate\n",
    "            if val_loss > 0.99 * val_loss_old:\n",
    "                learning_rate /= decay_rate\n",
    "                optimizer = update_learning_rate(optimizer, learning_rate)\n",
    "            val_loss_old = val_loss\n",
    "\n",
    "        if epoch % test_every == 0 or epoch == max_epochs - 1:\n",
    "            # Test\n",
    "            test_time, test_loss, _, _, _, test_pred_tour_len, test_gt_tour_len = \\\n",
    "                test(net, config, epoch_bar, mode='test')\n",
    "            epoch_bar.write('T: ' + metrics_to_str(epoch, test_time, learning_rate, test_loss,\n",
    "                                                    0, 0, 0, test_pred_tour_len, test_gt_tour_len))\n",
    "            writer.add_scalar('loss/test_loss', test_loss, epoch)\n",
    "            writer.add_scalar('pred_tour_len/test_pred_tour_len', test_pred_tour_len, epoch)\n",
    "\n",
    "        # Checkpoint √† chaque epoch\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': net.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_loss if 'val_loss' in dir() else 0,\n",
    "        }, log_dir + \"last_train_checkpoint.tar\")\n",
    "\n",
    "        # Checkpoint toutes les 250 epochs\n",
    "        if epoch != 0 and (epoch % 250 == 0 or epoch == max_epochs - 1):\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': net.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_loss': train_loss,\n",
    "                'val_loss': val_loss if 'val_loss' in dir() else 0,\n",
    "            }, log_dir + f\"checkpoint_epoch{epoch}.tar\")\n",
    "\n",
    "    writer.close()\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Lancer l'entra√Ænement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = main(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Charger le meilleur checkpoint et √©valuer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le meilleur checkpoint\n",
    "log_dir = f\"/kaggle/working/logs/{config.expt_name}/\"\n",
    "if torch.cuda.is_available():\n",
    "    checkpoint = torch.load(log_dir + \"best_val_checkpoint.tar\")\n",
    "else:\n",
    "    checkpoint = torch.load(log_dir + \"best_val_checkpoint.tar\", map_location='cpu')\n",
    "\n",
    "net.load_state_dict(checkpoint['model_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "train_loss = checkpoint['train_loss']\n",
    "val_loss = checkpoint['val_loss']\n",
    "print(f\"Checkpoint charg√© depuis l'epoch {epoch}\")\n",
    "print(f\"  Train loss: {train_loss:.4f}\")\n",
    "print(f\"  Val loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. Visualisation des pr√©dictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()\n",
    "\n",
    "viz_batch_size = 10\n",
    "num_nodes = config.num_nodes\n",
    "num_neighbors = config.num_neighbors\n",
    "beam_size = config.beam_size\n",
    "test_filepath = config.test_filepath\n",
    "\n",
    "dataset = iter(GoogleTSPReader(num_nodes, num_neighbors, viz_batch_size, test_filepath))\n",
    "batch = next(dataset)\n",
    "\n",
    "with torch.no_grad():\n",
    "    x_edges = Variable(torch.LongTensor(batch.edges).type(dtypeLong), requires_grad=False)\n",
    "    x_edges_values = Variable(torch.FloatTensor(batch.edges_values).type(dtypeFloat), requires_grad=False)\n",
    "    x_nodes = Variable(torch.LongTensor(batch.nodes).type(dtypeLong), requires_grad=False)\n",
    "    x_nodes_coord = Variable(torch.FloatTensor(batch.nodes_coord).type(dtypeFloat), requires_grad=False)\n",
    "    y_edges = Variable(torch.LongTensor(batch.edges_target).type(dtypeLong), requires_grad=False)\n",
    "\n",
    "    edge_labels = y_edges.cpu().numpy().flatten()\n",
    "    edge_cw = compute_class_weight(\"balanced\", classes=np.unique(edge_labels), y=edge_labels)\n",
    "    print(f\"Class weights: {edge_cw}\")\n",
    "\n",
    "    y_preds, loss = net.forward(x_edges, x_edges_values, x_nodes, x_nodes_coord, y_edges, edge_cw)\n",
    "    loss = loss.mean()\n",
    "\n",
    "    bs_nodes = beamsearch_tour_nodes_shortest(\n",
    "        y_preds, x_edges_values, beam_size, viz_batch_size, num_nodes,\n",
    "        dtypeFloat, dtypeLong, probs_type='logits')\n",
    "\n",
    "    pred_tour_len = mean_tour_len_nodes(x_edges_values, bs_nodes)\n",
    "    gt_tour_len = np.mean(batch.tour_len)\n",
    "    print(f\"Tour pr√©dit (moyenne): {pred_tour_len:.3f}\")\n",
    "    print(f\"Tour GT (moyenne):     {gt_tour_len:.3f}\")\n",
    "    print(f\"Gap d'optimalit√©:      {(pred_tour_len/gt_tour_len - 1)*100:.2f}%\")\n",
    "\n",
    "    # V√©rification de validit√©\n",
    "    for idx, nodes in enumerate(bs_nodes):\n",
    "        if not is_valid_tour(nodes, num_nodes):\n",
    "            print(f\"  Tour invalide #{idx}: {nodes}\")\n",
    "\n",
    "    # Visualiser\n",
    "    plot_predictions_beamsearch(x_nodes_coord, x_edges, x_edges_values, y_edges,\n",
    "                                y_preds, bs_nodes, num_plots=viz_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18. √âvaluation finale (Greedy, Beam Search, BS*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = config.learning_rate\n",
    "epoch_bar = master_bar(range(epoch + 1, epoch + 2))\n",
    "config.batch_size = 200  # Plus grand batch pour √©valuation\n",
    "\n",
    "# Utiliser le test set pour toutes les √©valuations\n",
    "config_eval = Settings(dict(config))\n",
    "config_eval.val_filepath = config.test_filepath\n",
    "\n",
    "for ep in epoch_bar:\n",
    "    # Greedy search (beam_size=1)\n",
    "    config_eval.beam_size = 1\n",
    "    t = time.time()\n",
    "    val_time, val_loss, _, _, _, val_pred_tour_len, val_gt_tour_len = test(net, config_eval, epoch_bar, mode='val')\n",
    "    print(f\"Greedy time: {time.time()-t:.1f}s\")\n",
    "    epoch_bar.write('Greedy: ' + metrics_to_str(ep, val_time, learning_rate, val_loss,\n",
    "                                                 0, 0, 0, val_pred_tour_len, val_gt_tour_len))\n",
    "\n",
    "    # Vanilla beam search\n",
    "    config_eval.beam_size = 1280\n",
    "    t = time.time()\n",
    "    val_time, val_loss, _, _, _, val_pred_tour_len, val_gt_tour_len = test(net, config_eval, epoch_bar, mode='val')\n",
    "    print(f\"BS time: {time.time()-t:.1f}s\")\n",
    "    epoch_bar.write('BS:     ' + metrics_to_str(ep, val_time, learning_rate, val_loss,\n",
    "                                                 0, 0, 0, val_pred_tour_len, val_gt_tour_len))\n",
    "\n",
    "    # Beam search with shortest tour heuristic\n",
    "    config_eval.beam_size = 1280\n",
    "    t = time.time()\n",
    "    test_time, test_loss, _, _, _, test_pred_tour_len, test_gt_tour_len = test(net, config_eval, epoch_bar, mode='test')\n",
    "    print(f\"BS* time: {time.time()-t:.1f}s\")\n",
    "    epoch_bar.write('BS*:    ' + metrics_to_str(ep, test_time, learning_rate, test_loss,\n",
    "                                                 0, 0, 0, test_pred_tour_len, test_gt_tour_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Notes:**\n",
    "- Le mod√®le utilise `nn.DataParallel` pour distribuer automatiquement les batches sur les 2 GPU Kaggle.\n",
    "- Les checkpoints sont sauvegard√©s dans `/kaggle/working/logs/<expt_name>/`.\n",
    "- Modifiez `TSP_SIZE` dans la cellule 10 pour basculer entre TSP10 et TSP20.\n",
    "- Les donn√©es doivent √™tre dans `/kaggle/input/` (ajoutez le dataset via l'interface Kaggle)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
